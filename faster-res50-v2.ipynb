{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":50870,"databundleVersionId":5392924,"sourceType":"competition"}],"dockerImageVersionId":30474,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pathlib\nimport os\nimport pandas as pd\nfrom pathlib import Path\nimport cv2\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision\nfrom torchvision import datasets, transforms\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torchvision.transforms import functional as F\nfrom torchvision.transforms import ToTensor\nfrom torchvision.datasets import ImageFolder\nfrom PIL import Image","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-20T11:10:26.836777Z","iopub.execute_input":"2024-07-20T11:10:26.837608Z","iopub.status.idle":"2024-07-20T11:10:30.889562Z","shell.execute_reply.started":"2024-07-20T11:10:26.837567Z","shell.execute_reply":"2024-07-20T11:10:30.888769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tqdm\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-07-20T11:10:30.891288Z","iopub.execute_input":"2024-07-20T11:10:30.891791Z","iopub.status.idle":"2024-07-20T11:10:43.018520Z","shell.execute_reply.started":"2024-07-20T11:10:30.891763Z","shell.execute_reply":"2024-07-20T11:10:43.017308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class myDataset(Dataset):\n    def __init__(self, main_d, train=True,mask=False, transform=None):\n        self.split = \"train\" if train else \"test\"\n        self.dset_dir = Path(main_d)/self.split\n        self.transform = transform\n        self.files = []\n        self.mask=mask\n        self.df_mask=pd.DataFrame()\n        folders = sorted(os.listdir(self.dset_dir))\n        \n        if self.split==\"train\" :\n            if  mask:\n                self.df_mask=pd.read_csv(main_d+\"train.csv\")\n            for folder in folders:\n                class_idx= folders.index(folder)\n                folder_dir = self.dset_dir/folder\n                files = os.listdir(folder_dir)\n                self.files += [{\"mask\":folder+\"/\"+x,\"file\": folder_dir/x, \"class\": class_idx+1} for x in files]\n                \n        else:\n            self.file=folders\n            for file in folders:\n                 self.files.append(self.dset_dir/file)\n    \n    def __len__(self):\n        return len(self.files)\n    \n    def __getitem__(self, i):\n        \n        if self.split == \"train\":\n            item = self.files[i]\n            file = item['file']\n            \n            # reading the images and converting them to correct size and color    \n            img = cv2.imread(str(file))\n            img_res = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n            img_res /= 255.0\n            wt = 350\n            ht = 350\n            \n            # recover bounding boxes\n            name_file_img=item['mask']\n            mask_data=self.df_mask[self.df_mask[\"image\"]==name_file_img]\n            xmin=int(mask_data[\"x1\"])\n            ymin=int(mask_data[\"y1\"])\n            xmax=int(mask_data[\"x2\"])\n            ymax=int(mask_data[\"y2\"])\n            \n            # resize bounding boxes\n            boxes = []\n            xmin_corr = xmin+1\n            xmax_corr = xmax-1\n            ymin_corr = ymin+1\n            ymax_corr = ymax-1\n            \n            boxes.append([xmin_corr, ymin_corr, xmax_corr, ymax_corr])\n            # convert boxes into a torch.Tensor\n            boxes = torch.as_tensor(boxes, dtype=torch.int64)\n            \n            # getting the areas of the boxes\n            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n            \n\n            # suppose all instances are not crowd\n            iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n           \n            \n            labels = torch.tensor(item['class'],dtype=torch.int64)\n            labels=labels.unsqueeze(0)\n            \n            target = {}\n            target[\"boxes\"] = boxes\n            target[\"labels\"] = labels\n            target[\"area\"] = area\n            target[\"iscrowd\"] = iscrowd\n           \n            # image_id\n            target[\"image_id\"] = torch.tensor([i])\n            \n            if self.transform:\n            \n                sample = {'image' : img_res,\n                          'bboxes' : target['boxes'],\n                          'labels' : labels\n                         }\n               \n                sample = self.transform(**sample)\n                img_res = sample['image']\n               \n                target['boxes'] = torch.as_tensor((sample['bboxes']),dtype=torch.int64)\n                \n                \n               \n            return img_res, target\n        else:\n            file = self.files[i]\n            \n            \n            # reading the images and converting them to correct size and color    \n            img = cv2.imread(str(file))\n            img_res = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n            # diving by 255\n            img_res /= 255.0\n            \n            if self.transform:\n                sample = {\n                    'image': img_res,\n                }\n                sample = self.transform(**sample)\n                image = sample['image']\n            return image,self.file[i]\n        ","metadata":{"execution":{"iopub.status.busy":"2024-07-20T11:10:43.020114Z","iopub.execute_input":"2024-07-20T11:10:43.020419Z","iopub.status.idle":"2024-07-20T11:10:43.043339Z","shell.execute_reply.started":"2024-07-20T11:10:43.020388Z","shell.execute_reply":"2024-07-20T11:10:43.042457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main_d = \"../input/aiunict-2023/\"","metadata":{"execution":{"iopub.status.busy":"2024-07-20T11:10:43.046359Z","iopub.execute_input":"2024-07-20T11:10:43.046660Z","iopub.status.idle":"2024-07-20T11:10:43.054252Z","shell.execute_reply.started":"2024-07-20T11:10:43.046637Z","shell.execute_reply":"2024-07-20T11:10:43.053264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom albumentations.core.transforms_interface import ImageOnlyTransform\n\ndef train_transf():\n    return A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.RandomBrightnessContrast(p=0.5),\n        A.GaussianBlur(p=0.3),\n        A.Rotate(limit=15, p=0.5),\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n\ndef test_transf():\n    return A.Compose([\n        ToTensorV2(p=1.0)\n    ])","metadata":{"execution":{"iopub.status.busy":"2024-07-20T11:10:43.055634Z","iopub.execute_input":"2024-07-20T11:10:43.055960Z","iopub.status.idle":"2024-07-20T11:10:43.996368Z","shell.execute_reply.started":"2024-07-20T11:10:43.055936Z","shell.execute_reply":"2024-07-20T11:10:43.995510Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = myDataset(main_d, train=True, mask=True, transform=train_transf())\ndef collate_fn(batch):\n    return tuple(zip(*batch))\ntrain_dl = DataLoader(train, batch_size = 8, shuffle = True, num_workers = 4, pin_memory=True,collate_fn= collate_fn)\ntest_set = myDataset(main_d, train=False, transform= test_transf())","metadata":{"execution":{"iopub.status.busy":"2024-07-20T11:10:43.997667Z","iopub.execute_input":"2024-07-20T11:10:43.998017Z","iopub.status.idle":"2024-07-20T11:10:44.277687Z","shell.execute_reply.started":"2024-07-20T11:10:43.997990Z","shell.execute_reply":"2024-07-20T11:10:44.276698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\n\nimages, targets = next(iter(train_dl))\nindices = random.sample(range(len(images)), 8)\nfig, axs = plt.subplots(2, 4, figsize=(15, 10))\n\n# Iterate over the sampled indices and plot images with bounding boxes\nfor idx, i in enumerate(indices):\n    ax = axs[idx // 4, idx % 4]\n    image = images[i].permute(1, 2, 0).numpy()\n    ax.imshow(image)\n    \n    if \"boxes\" in targets[i]:\n        bboxes = targets[i][\"boxes\"]\n        for bbox in bboxes:\n            xmin, ymin, xmax, ymax = bbox.tolist()\n            rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, linewidth=1, edgecolor='r', facecolor='none')\n            ax.add_patch(rect)\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-20T11:10:44.278994Z","iopub.execute_input":"2024-07-20T11:10:44.279345Z","iopub.status.idle":"2024-07-20T11:10:49.456974Z","shell.execute_reply.started":"2024-07-20T11:10:44.279312Z","shell.execute_reply":"2024-07-20T11:10:49.455506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\nmodel = fasterrcnn_resnet50_fpn_v2(weights='DEFAULT')  # Using the updated model version with default weights\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nnum_classes = 9  #(1 background + 8 object classes)\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n# Move the model to the device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-20T11:10:52.082609Z","iopub.execute_input":"2024-07-20T11:10:52.082938Z","iopub.status.idle":"2024-07-20T11:11:03.108307Z","shell.execute_reply.started":"2024-07-20T11:10:52.082906Z","shell.execute_reply":"2024-07-20T11:11:03.107350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.optim.lr_scheduler import OneCycleLR\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\nlr_scheduler = OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(train_dl), epochs=5)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-20T11:11:03.111304Z","iopub.execute_input":"2024-07-20T11:11:03.111592Z","iopub.status.idle":"2024-07-20T11:11:03.118349Z","shell.execute_reply.started":"2024-07-20T11:11:03.111568Z","shell.execute_reply":"2024-07-20T11:11:03.117428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Averager:\n    def __init__(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n\n    def send(self, value):\n        self.current_total += value\n        self.iterations += 1\n\n    @property\n    def value(self):\n        if self.iterations == 0:\n            return 0\n        else:\n            return 1.0 * self.current_total / self.iterations\n\n    def reset(self):\n        self.current_total = 0.0\n        self.iterations = 0.0","metadata":{"execution":{"iopub.status.busy":"2024-07-20T11:11:03.119586Z","iopub.execute_input":"2024-07-20T11:11:03.119965Z","iopub.status.idle":"2024-07-20T11:11:03.129647Z","shell.execute_reply.started":"2024-07-20T11:11:03.119939Z","shell.execute_reply":"2024-07-20T11:11:03.128897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nnum_epochs = 6  \nloss_hist = Averager()\nbest_loss = float('inf')\nitr = 1\nlossHistoryiter = []\nlossHistoryepoch = []\n\nstart = time.time()\n\nfor epoch in range(num_epochs):\n    loss_hist.reset()\n    model.train()  \n\n    # Wrap the data loader with tqdm to add a progress bar\n    for images, targets in tqdm(train_dl, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\"):\n        # Move images and targets to the CUDA device\n        images = [image.to(device) for image in images]\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        \n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n        loss_value = losses.item()\n        \n        #loss_hist.send(loss_value)\n        lossHistoryiter.append(loss_value)\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n        \n        if itr % 50 == 0:\n            print(f\"Iteration #{itr} loss: {loss_value}\")\n        itr += 1\n        \n        # update the learning rate\n    if lr_scheduler is not None:\n        lr_scheduler.step()\n        \n    lossHistoryepoch.append(loss_hist.value)\n    print(f\"Epoch #{epoch} loss: {loss_hist.value}\")\n    \ntorch.save(model, \"/kaggle/working/model2.pth\")\n\nend = time.time()\nhours, rem = divmod(end-start, 3600)\nminutes, seconds = divmod(rem, 60)\nprint(f\"Time taken to Train the model: {int(hours):0>2}:{int(minutes):0>2}:{seconds:05.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-20T11:11:03.130830Z","iopub.execute_input":"2024-07-20T11:11:03.131159Z","iopub.status.idle":"2024-07-20T13:38:06.956017Z","shell.execute_reply.started":"2024-07-20T11:11:03.131135Z","shell.execute_reply":"2024-07-20T13:38:06.954805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def apply_nms(orig_prediction, iou_thresh=0.3):\n    keep = torchvision.ops.nms(orig_prediction['boxes'], orig_prediction['scores'], iou_thresh)\n    final_prediction = orig_prediction\n    final_prediction['boxes'] = final_prediction['boxes'][keep]\n    final_prediction['scores'] = final_prediction['scores'][keep]\n    final_prediction['labels'] = final_prediction['labels'][keep]\n    \n    return final_prediction","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:38:06.957701Z","iopub.execute_input":"2024-07-20T13:38:06.958499Z","iopub.status.idle":"2024-07-20T13:38:06.965081Z","shell.execute_reply.started":"2024-07-20T13:38:06.958462Z","shell.execute_reply":"2024-07-20T13:38:06.964072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_id=[]\nimage_class=[]\nfor idx in range(test_set.__len__()):\n    img,name_file = test_set[idx]\n    model.eval()\n    with torch.no_grad():\n        prediction = model([img.to(device)])[0]\n    nms_prediction = apply_nms(prediction, iou_thresh=0.2)\n    pred=nms_prediction['labels'].cpu().numpy()[0]\n    \n    image_id.append(name_file)\n    image_class.append(pred-1)\n    \nd = {'image': image_id, 'class': image_class}\ndf = pd.DataFrame(data=d)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:38:06.974821Z","iopub.execute_input":"2024-07-20T13:38:06.975107Z","iopub.status.idle":"2024-07-20T13:40:31.865542Z","shell.execute_reply.started":"2024-07-20T13:38:06.975084Z","shell.execute_reply":"2024-07-20T13:40:31.864421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv(\"submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T13:40:31.867123Z","iopub.execute_input":"2024-07-20T13:40:31.867535Z","iopub.status.idle":"2024-07-20T13:40:31.878785Z","shell.execute_reply.started":"2024-07-20T13:40:31.867498Z","shell.execute_reply":"2024-07-20T13:40:31.877775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
